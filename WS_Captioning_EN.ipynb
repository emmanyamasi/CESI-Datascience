{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTvA9MTuKv0Q"
      },
      "source": [
        "# <center>Image annotation<br /> Workshop</center>\n",
        "\n",
        "![logoEI_1_6.jpg](attachment:logoEI_1_6.jpg)\n",
        "Designer : Nassim HADDAM\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrUKE6YaKv0S"
      },
      "source": [
        "The aim of this workshop is to give you the skills you need to carry out the [image annotation] workshop (https://fr.wikipedia.org/wiki/Annotation_automatique_d%27images) (or image subtitling). In this workshop, you'll be using CNNs (to create attributes useful for annotation) and RNNs (to do the actual annotation). You'll also need to do **a lot** of pre-processing. This workshop will focus on this part of the process, and will give you an idea of the network architecture that will be used for annotation. We'll start by importing the libraries we're interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mabb8l2IKv0T"
      },
      "source": [
        "You must therefore re-execute the cells corresponding to data preparation in the previous workshop, from data loading to image and annotation pre-processing.\n",
        "<br><br>\n",
        "<b>Imports</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az4-Xd2dKv0T"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiVkRGC9Kv0U"
      },
      "source": [
        "<b>Chargement des données</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3LqdMQvKv0U"
      },
      "outputs": [],
      "source": [
        "# Annotation file path\n",
        "annotation_folder = \"/annotations/\"\n",
        "annotation_file = os.path.abspath('.')+\"/annotations/captions_train2014.json\"\n",
        "\n",
        "# Path to folder containing images to be annotated\n",
        "image_folder = '/train2014/'\n",
        "PATH = os.path.abspath('.') + image_folder\n",
        "\n",
        "# Read annotation file\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Group all annotations with the same identifier.\n",
        "image_path_to_caption = collections.defaultdict(list)\n",
        "for val in annotations['annotations']:\n",
        "    # mark the beginning and end of each annotation\n",
        "# PLEASE COMPLETE\n",
        "    # The image ID is part of the image path\n",
        "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
        "    # Add caption associated with image_path\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "# Take first images only\n",
        "image_paths = list(image_path_to_caption.keys())\n",
        "train_image_paths = image_paths[:2000]\n",
        "\n",
        "# List all annotations\n",
        "train_captions = []\n",
        "# List of all duplicated image filenames (in number of annotations per image)\n",
        "img_name_vector = []\n",
        "\n",
        "for image_path in train_image_paths:\n",
        "    caption_list = image_path_to_caption[image_path]\n",
        "    # Add caption_list to train_captions\n",
        "#PLEASE COMPLETE\n",
        "    # Add duplicated image_path len(caption_list) times\n",
        "#PLEASE COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwO35kZSKv0V"
      },
      "outputs": [],
      "source": [
        "# Download the pre-trained InceptionV3 model with cassification from ImageNet\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "# Creation of a variable that will be the input to the new image pre-processing model\n",
        "new_input = \\\n",
        "# PLEASE COMPLETE\n",
        "# retrieve the last hidden layer containing the image in compact representation\n",
        "hidden_layer = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "# Model that calculates a dense representation of images with InceptionV3\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "# Definition of load_image function\n",
        "def load_image(image_path):\n",
        "    \"\"\"\n",
        "    The load_image function has as input the path of an image and as output a pair\n",
        "    containing the processed image and its path.\n",
        "    The load_image function performs the following processing:\n",
        "        1. Loads the file corresponding to the path image_path\n",
        "        2. Decodes the image into RGB.\n",
        "        3. Resize image to size (299, 299).\n",
        "        4. Normalize image pixels between -1 and 1.\n",
        "    \"\"\"\n",
        "    img = \\\n",
        "#PLEASE COMPLETE\n",
        "    img = \\\n",
        "#PLEASE COMPLETE\n",
        "    img = \\\n",
        "#PLEASE COMPLETE\n",
        "    img = \\\n",
        "#PLEASE COMPLETE\n",
        "    return img, image_path\n",
        "\n",
        "# Image pre-processing\n",
        "# Take image names\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Create an instance of \"tf.data.Dataset\" based on image names\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "# Data split into batches after pre-processing by load_image\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "# Browse dataset batch by batch for InceptionV3 pre-processing\n",
        "for img, path in tqdm(image_dataset):\n",
        "    # InceptionV3 pre-processes current batch (size (16,8,8,2048))\n",
        "    batch_features = image_features_extract_model(img)\n",
        "    # Resize batch size (16,8,8,2048) to (16,64,2048)\n",
        "    batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "    # Browse current batch and store path and batch with np.save()\n",
        "    for bf, p in zip(batch_features, path):\n",
        "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "        # (image path associated with its new representation , image representation)\n",
        "        np.save(path_of_feature, bf.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeNQ-e1DKv0V"
      },
      "source": [
        "**Pré-traitement des annotations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2n_zQfHKv0V"
      },
      "outputs": [],
      "source": [
        "# Find the maximum size\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "# Select the 5000 most frequent words in the vocabulary\n",
        "top_k = 5000\n",
        "#The Tokenizer class enables text pre-processing for neural networks\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "# Builds a vocabulary based on the train_captions list\n",
        "tokenizer.fit_on_texts(\\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "# Create token to fill annotations to equalize length\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'] = 0\n",
        "\n",
        "# Creation of vectors (list of integer tokens) from annotations (list of words)\n",
        "train_seqs = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "# Fill each vector up to maximum annotation length\n",
        "cap_vector = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "# Calculates the maximum length used to store attention weights\n",
        "# This will be used later for display during evaluation\n",
        "max_length = calc_max_length(train_seqs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ0_RvG6Kv0V"
      },
      "source": [
        "# 1 Training and test set formation :\n",
        "Next, you need to separate the dataset into two parts: a training set and a test set. The code that performs these operations is detailed in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgqVC3LpKv0W"
      },
      "outputs": [],
      "source": [
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "# Creation of a dictionary associating image paths with (.npy file) annotationss\n",
        "# Images are duplicated because there are several annotations per image\n",
        "print(len(img_name_vector), len(cap_vector))\n",
        "for img, cap in zip(img_name_vector, cap_vector):\n",
        "    img_to_cap_vector[img].append(cap)\n",
        "\n",
        "\"\"\"\n",
        "Creation of training and validation datasets using\n",
        "random 80-20 splitting\n",
        "\"\"\"\n",
        "# Take the keys (names of processed image files), *these will not be duplicated*.\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "#PLEASE COMPLETE\n",
        "# Split indexes into training and test\n",
        "slice_index = int(len(img_keys)*0.8)\n",
        "img_name_train_keys, img_name_val_keys = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "\"\"\"\n",
        "Training and test sets are in the form of\n",
        "lists containing mappings:(pre-processed image ---> annotation token(word) )\n",
        "\"\"\"\n",
        "\n",
        "# Loop to build training set\n",
        "img_name_train = []\n",
        "cap_train = []\n",
        "for imgt in img_name_train_keys:\n",
        "    capt_len = len(img_to_cap_vector[imgt])\n",
        "    # Duplicate images by number of annotations per image\n",
        "    img_name_train.extend([imgt] * capt_len)\n",
        "    cap_train.extend(img_to_cap_vector[imgt])\n",
        "\n",
        "# Loop to build test set\n",
        "img_name_val = []\n",
        "cap_val = []\n",
        "for imgv in img_name_val_keys:\n",
        "    capv_len = \\\n",
        "#PLEASE COMPLETE\n",
        "    # Duplication of images in the number of annotations per image\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kf3s79pKv0W"
      },
      "source": [
        "Creation of a training dataset represented by an instance [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) starting from the base dataset (the file names and annotations of the training dataset). The `tf.data.Dataset` class is used to represent large datasets and facilitate their pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKMvPKPtKv0W"
      },
      "outputs": [],
      "source": [
        "# Feel free to modify these parameters to suit your machine\n",
        "BATCH_SIZE = 64 # batch size\n",
        "BUFFER_SIZE = 1000 # buffer size for data mixing\n",
        "embedding_dim = 256\n",
        "units = 512 # size of hidden layer in RNN\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "\n",
        "# The shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# The following two variables represent the shape of this vector\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "\n",
        "# Function to load numpy files from pre-processed images\n",
        "def map_func(img_name, cap):\n",
        "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "    return img_tensor, cap\n",
        "\n",
        "# Creation of a \"Tensor \"s dataset (used to represent large datasets)\n",
        "# The dataset is created from \"img_name_train\" and \"cap_train\".\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load numpy files (possibly in parallel)\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Mix data and divide into batches\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e08RCv9Kv0X"
      },
      "source": [
        "acc# 2 The model :\n",
        "Compared with the model, the last convolutional layer of `InceptionV3` is of the form `(8, 8, 2048)`. This vector was reshaped into the form `(64, 2048)` during disk-level storage. This vector is then passed through the CNN encoder (which consists of a single, fully connected layer). The RNN will predict the next word in the annotation for this vector. [The image](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/22_Image_Captioning.ipynb) below shows an example of a very basic annotation system architecture using a CNN and an RNN.\n",
        "\n",
        "![22_image_captioning_flowchart.png](attachment:22_image_captioning_flowchart.png)\n",
        "\n",
        "In this example the captioning is done this way:\n",
        "<ul>\n",
        "    <li>The image is passed through the CNN to get a compact representation of it. This representation is returned by the `Dense 2` layer of size 4096.</li>\n",
        "    <li>This representation is reduced by passing it to the dense layer `Dense Map` to be input as the initial hidden state to the RNN cells.</li>\n",
        "    <li>The RNN part is composed of <a href=\"https://openclassrooms.com\">GRU</a> (Gated Reccurent Unit). This part is made up of 3 layers. One layer represents a level of abstraction of the language. </li>\n",
        "    <li>The RNN has as input the annotation as well as the image in compact form, and returns for each column the word following the input word at column level.</li>\n",
        "    <li>The annotations are represented as a word list. This list is unusable by RNN, so it's passed to a module that replaces each word with an integer (or integer token), and then to another module that projects each token into a vector whose elements are between -1 and 1.</li>\n",
        "</ul>\n",
        "\n",
        "Your annotation system will follow, broadly speaking, the same principle as shown in the image below, nevertheless it will contain some essential differences distinguishing it from this example. In particular, the system will contain an attention mechanism (explained in [article](https://arxiv.org/pdf/1502.03044.pdf) and in [the video](https://www.youtube.com/watch?v=uCSTpOLMC48&list=WL&index=257)) whose function is to cause the neural network to give greater importance in its annotation predictions to the most telling and relevant parts of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWVfnl5PKv0X"
      },
      "source": [
        "**The CNN encoder:**\n",
        "\n",
        "The CNN encoder produces a suitable representation of the image, which it passes on to the RNN decoder for captioning. The CNN has as input the characteristics of images already pre-processed by InceptionV3 and stored on disk.\n",
        "\n",
        "Please note that in the CNN part of this neural network, the last convolutional layer is not flattened, as in the previous workshop on CNNs. Remember that the images from InceptionV3 pre-processing were of the form 8x8x2048. These images have now been resized to 64x2048. This means that for each of the 64 positions of the pre-processed image, this representation contains the 2048 features extracted by InveptoinV3. And so, the input to the CNN decoder is a batch where each element is made up of the 2048 features of the 64 positions of the pre-processed image (which was originally 8x8). The dense layer that follows calculates a new image representation of size 64x256, where each image position has 256 features. The weights are the same for neurons of the same position in the same column in the pre-processed image (which are associated with the same image feature). This is due to the way the [dense layer]( https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) handles matrix operations in tensorflow.\n",
        "\n",
        "The advantage of this representation over the flattened representation is that it preserves spatial information at the level of the neural network layers. This will enable the attention mechanism of the RNN part to detect interesting positions in the image and inform the algorithm on which zone it should place the most importance when captioning the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE8KIJTJKv0X"
      },
      "outputs": [],
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since the images are already preprocessed by InceptionV3, they are represented in compact form.\n",
        "    # The CNN encoder will simply transmit these characteristics to a dense layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # form after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = \\\n",
        "#PLEASE COMPLETE\n",
        "        x = \\\n",
        "#PLEASE COMPLETE\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtOR-eJEKv0X"
      },
      "source": [
        "**The attention mechanism:**\n",
        "\n",
        "The attention mechanism is very similar to a [classical RNN] cell( https://fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_r%C3%A9currents), but with a few differences. The attention part has as input the representation of the pre-processed image returned by the CNN as well as the current value of the hidden state of the GRU, and as output the **context vector** which reflects the most important features of the image. An intermediate step in calculating this vector is to compute the **attention weights**, which represent the importance of each image position (of which there are 64) in predicting its annotation.\n",
        "\n",
        "The image representation given as input is initially transformed in the same way as for CNN, by passing it to a dense layer of size `units`. Similarly, the hidden state is also passed to a dense layer of size `units`. The new image representation is then added to the hidden state and passed to an activation function of type [`tanh`](https://fr.wikipedia.org/wiki/Tangente_hyperbolique) as for conventional RNN cells. At this level, we'll have a data representation of size `64xunits` containing a mixture of information about the image and the annotation text. A score is then assigned to each position by passing this representation to a dense layer. These scores are normalized with a softmax layer to produce the vector of **attention weights**.\n",
        "\n",
        "Finally, each feature of the input image representation is multiplied (weighted) by the attention vector. After this, the sum of each feature along the positions (the lines of the representation) is taken to form the **context vector**.\n",
        "\n",
        "Overall, we can say that the attention vector depends on scores that are learned from a spatial and textual representation of the image. This attention vector reflects the relevance of each position and is used to calculate the context vector, which will give us the importance of the image's features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ-tDNsUKv0X"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = \\\n",
        "#PLEASE COMPLETE\n",
        "        self.W2 = \\\n",
        "#PLEASE COMPLETE\n",
        "        self.V = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "        # hidden layer shape == (batch_size, hidden_size)\n",
        "        hidden_with_time_axis = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        attention_hidden_layer = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        # This gives you a non-normalized score for each image feature.\n",
        "        score = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        attention_weights = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        context_vector = \\\n",
        "#PLEASE COMPLETE\n",
        "        context_vector = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9cB4jzeKv0Y"
      },
      "source": [
        "**The RNN decoder:**\n",
        "\n",
        "The role of the RNN decoder is to use the pre-processed representation of the image to predict its caption word by word. This RNN has a single cell type [GRU]( https://en.wikipedia.org/wiki/Gated_recurrent_unit). The GRU has a hidden state that represents the memory of the last elements seen by it. The GRU updates its state before returning it, to do this it uses certain memory mechanisms which are quite sophisticated.\n",
        "\n",
        "The decoder is structured as follows: each time the RNN is called, the current word, the image representation and the GRU's hidden state are given as input to the RNN. As words are represented by integers, they must be passed through an embedding layer (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), which calculates a vector representation of size `output_dim` from the number representing the word.\n",
        "\n",
        "In addition, the attention mechanism provides a vector representing **the context** of the image, i.e. a vector that tells us about the image's dominant features. This vector is computed by a call to the attention mechanism, providing it with the image features encoded by the CNN and the hidden state of the GRU, which summarizes the history of words seen by the RNN so far.\n",
        "\n",
        "Next, the current word and context are concatenated to form the GRU's input vector, which in turn calculates the state in the next step. This state is passed through a dense layer of size `units`, then the output of this layer is passed to another dense layer of size `vocab_size`, which returns the score associated with each word in the vocabulary in order to predict the next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xje_nT3fKv0Y"
      },
      "outputs": [],
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = \\\n",
        "#PLEASE COMPLETE\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        #Dense layer with GRU output as input\n",
        "        self.fc1 = \\\n",
        "#PLEASE COMPLETE\n",
        "        # Last dense layer\n",
        "        self.fc2 = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        self.attention = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # Attention is defined by a separate model\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "        # Pass current word to embedding layer\n",
        "        x = \\\n",
        "#PLEASE COMPLETE\n",
        "        # Concatenation\n",
        "        x = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        # Passage of the concatenated vector to the gru\n",
        "        output, state = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        # Dense layer\n",
        "        y = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        y = tf.reshape(y, (-1, x.shape[2]))\n",
        "\n",
        "        # Dense layer\n",
        "        y = \\\n",
        "#PLEASE COMPLETE\n",
        "\n",
        "        return y, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB8xb_bgKv0Y"
      },
      "source": [
        "**Combiner la partie encodeur et décodeur :**\n",
        "\n",
        "Vous devez compléter la partie du code pour la création de l'encodeur et du décodeur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WreuzUyXKv0Y"
      },
      "outputs": [],
      "source": [
        "# Encoder creation\n",
        "encoder = \\\n",
        "#PLEASE COMPLETE\n",
        "# Decoder creation\n",
        "decoder = \\\n",
        "#PLEASE COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C8XrpVNKv0Y"
      },
      "outputs": [],
      "source": [
        "# ADAM optimizer\n",
        "optimizer = \\\n",
        "#PLEASE COMPLETE\n",
        "# The loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFcZOgN1Kv0Y"
      },
      "source": [
        "To keep track of your learning and save it, you can use the class [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owLSJLUWKv0Y"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pu6Q_ZEKv0Z"
      },
      "source": [
        "Initialize training start time in `start_epoch`. The `tf.train.Checkpoint` class allows you to continue training where you left off if it was interrupted earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OBTqr5BKv0Z"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    # Restore last checkpoint in checkpoint_path\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prgYEAqTKv0Z"
      },
      "source": [
        "# 3 Training and testing:\n",
        "Next, you'll implement the `train_step` and `evaluate` functions:\n",
        "\n",
        "- The `train_step` function represents a step in network training. It consists of the encoder's evaluation of the vector pre-calculated by InceptionV3. The output of this step is transmitted to the decoder, which then predicts the annotation word by word. The loop for predicting each word and calculating the associated loss should be implemented in this function.\n",
        "- The `evaluate` function will be used to evaluate the network's performance on the test set. It is therefore similar to the `train_step` function, except that the calculation part of the loss function is omitted, as it doesn't involve training the network.\n",
        "\n",
        "Finally, you need to implement the training and testing part of the code. Note that training is done here by image batch.\n",
        "\n",
        "# 3.1 Training\n",
        "The function used to complete a training step on a batch of images is `train_step`. The function has as input a batch of pre-processed images and their annotations, and returns the loss associated with this batch.\n",
        "\n",
        "The hidden state of the RNN part is initialized, as is the start word with the start token. The image features are then extracted by the encoder. After this, the batch is traversed word by word to predict the next word using the decoder. The decoder uses the hidden state, the image features and the previous word to predict the current word. The decoder updates the hidden state and returns it together with the batch predictions. The loss is calculated from the predictions returned by the decoder and the annotations associated with the batch.\n",
        "\n",
        "Finally, the overall loss and gradient are calculated and the network is updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgXl05EKKv0Z"
      },
      "outputs": [],
      "source": [
        "loss_plot = []\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "\n",
        "    # Initialize hidden state for each batch\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "    # Initialize decoder input\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape: # Enables loss gradient calculation\n",
        "        features = encoder(img_tensor)\n",
        "\n",
        "        for i in range(1, target.shape[1]):\n",
        "            # Prediction of the i'th word of the batch with the decoder\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "            # The correct word at step i is given as input at step (i+1)\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei1w8-AWKv0Z"
      },
      "source": [
        "The global code containing the training loop is shown below. This loop traverses the training dataset batch by batch and trains the network with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "dgFuTqtVKv0Z"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # saving loss\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "# Training curve display\n",
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAY0kevTKv0a"
      },
      "source": [
        "# 3.2 Test\n",
        "The function used to complete an evaluation step for testing is in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIyxsRYIKv0a"
      },
      "outputs": [],
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot\n",
        "\n",
        "# Function for representing attention at image level\n",
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiI-eOtFKv0a"
      },
      "source": [
        "L'affichage de quelques exemples sur le résultat retourné par l'évaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaOzsKoZKv0b"
      },
      "outputs": [],
      "source": [
        "# Display of some annotations in the test set\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "result, attention_plot = evaluate(image)\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image, result, attention_plot)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}